---
title: "UK2007 Spam Detection Analysis"
author: "Group 4"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    theme: united
    highlight: tango
    code_folding: show
---

# 1. General Description

## Dataset Overview

The WEBSPAM-UK2007 dataset was collected by the Web Algorithmics Lab at UniversitÃ  degli Studi di Milano to support the Web Spam Challenge 2008 competition. This dataset represents a comprehensive collection of web hosts from the .UK domain, aimed at developing and evaluating methods for detecting web spam.

## Dataset Purpose and Collection

-   **Collection Purpose**: To provide a standardized dataset for web spam detection research and competition
-   **Collection Entity**: Web Algorithmics Lab, UniversitÃ  degli Studi di Milano
-   **Domain Coverage**: .UK domain websites
-   **Primary Task**: Binary classification (spam vs. non-spam websites)

## Dataset Scale

-   **Total Hosts**: 114,529 websites in the .UK domain
-   **Labeled Samples**:
    -   Training Set (SET1): 3,998 samples
    -   Test Set (SET2): 2,055 samples
    -   Note: In this analysis, we combine both sets for comprehensive evaluation
-   **Domain Structure**: Based on third-level domains to prevent training/test contamination

## Label Categories

-   **Spam**: Websites identified as spam
-   **Non-spam**: Legitimate websites
-   **Undecided**: Uncertain cases (excluded from modeling)

# 2. General Properties

## Feature Sets Description

The dataset includes three distinct feature sets:

1.  **Direct Features (Obvious Features)**:
    -   Number of features: 2
    -   Types: Page count and domain name length
    -   Simple, directly observable characteristics
2.  **Link-based Features (Transformed)**:
    -   Number of features: 138
    -   Types: Various transformed link metrics
    -   Includes ratios and logarithmic transformations of link-based characteristics
3.  **Content-based Features**:
    -   Number of features: 96
    -   Types: Text and content characteristics
    -   Derived from website content analysis

> This chunk sets up the R environment and loads necessary libraries for data analysis, visualization, and reporting.

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 10,
  fig.height = 6
)

# Load required libraries
library(tidyverse)
library(skimr)
library(corrplot)
library(scales)
library(kableExtra)
```

> This chunk loads and preprocesses the dataset, including reading the feature sets and labels, combining the training and test sets, and merging features with their corresponding labels.

```{r load-data}
# Read the feature sets
direct_features <- read_csv("1.uk-2007-05.obvious_features.csv")
link_features <- read_csv("2.uk-2007-05.link_based_features_transformed.csv")
content_features <- read_csv("3.uk-2007-05.content_based_features.csv")

# Read and process labels - note these are space-separated files without headers
labels_set1 <- read.table("WEBSPAM-UK2007-SET1-labels.txt") %>%
  setNames(c("hostid", "label", "spamicity", "assessments"))

labels_set2 <- read.table("WEBSPAM-UK2007-SET2-labels.txt") %>%
  setNames(c("hostid", "label", "spamicity", "assessments"))

# Combine labels and convert to binary (spam/non-spam)
all_labels <- rbind(labels_set1, labels_set2) %>%
  filter(label != "undecided") %>%
  mutate(is_spam = ifelse(label == "spam", 1, 0))

# Merge features with labels
direct_with_labels <- direct_features %>%
  rename(hostid = `#hostid`) %>%
  inner_join(all_labels, by = "hostid") %>%
  select(-label, -spamicity, -assessments)

link_with_labels <- link_features %>%
  rename(hostid = `#hostid`) %>%
  inner_join(all_labels, by = "hostid") %>%
  select(-label, -spamicity, -assessments)

content_with_labels <- content_features %>%
  rename(hostid = `#hostid`) %>%
  inner_join(all_labels, by = "hostid") %>%
  select(-label, -spamicity, -assessments)
```

## Class Distribution Analysis

> This chunk analyzes and visualizes the distribution of spam vs non-spam labels in the dataset, highlighting the class imbalance issue.

```{r label-distribution}
# Analyze label distribution
label_dist <- all_labels %>%
  group_by(label) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

# Create a bar plot of label distribution
ggplot(label_dist, aes(x = label, y = count, fill = label)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = sprintf("%.1f%%", percentage)), 
            position = position_stack(vjust = 0.5)) +
  theme_minimal() +
  labs(title = "Distribution of Labels in the Dataset (Combined SET1 and SET2)",
       x = "Label",
       y = "Count",
       subtitle = "Shows significant class imbalance") +
  scale_fill_brewer(palette = "Set2")

# Print detailed distribution
cat("\nDetailed Label Distribution:\n")
cat("SET1 (Training):", 
    "\n- Non-spam:", sum(labels_set1$label == "nonspam"),
    "\n- Spam:", sum(labels_set1$label == "spam"),
    "\n\nSET2 (Testing):",
    "\n- Non-spam:", sum(labels_set2$label == "nonspam"),
    "\n- Spam:", sum(labels_set2$label == "spam"),
    "\n\nCombined:",
    "\n- Non-spam:", sum(all_labels$label == "nonspam"),
    "\n- Spam:", sum(all_labels$label == "spam"),
    "\n\nAnalysis: The dataset shows severe class imbalance, with spam samples being significantly underrepresented.")
```

## Feature Analysis and Standardization Requirements

> This chunk creates comprehensive summaries of each feature set, including the number of features, sample sizes, missing values, and value ranges to determine standardization requirements.

```{r feature-summaries}
# Function to create feature summary
create_feature_summary <- function(data, feature_set_name) {
  # Remove ID and label columns
  feature_cols <- data %>%
    select(-hostid, -is_spam) %>%
    # Remove hostname if it exists
    select(-any_of("hostname")) %>%
    names()
  
  # Calculate summary statistics
  summary_stats <- data %>%
    select(all_of(feature_cols)) %>%
    summary()
  
  # Check for missing values
  missing_count <- sum(is.na(data[feature_cols]))
  
  # Calculate value ranges
  value_ranges <- data %>%
    select(all_of(feature_cols)) %>%
    summarise(across(everything(), list(min = min, max = max))) %>%
    gather() %>%
    summarise(
      min_value = min(value),
      max_value = max(value)
    )
  
  # Combine into a data frame
  tibble(
    feature_set = feature_set_name,
    n_features = length(feature_cols),
    n_samples = nrow(data),
    missing_values = missing_count,
    value_range = sprintf("%.2f to %.2f", value_ranges$min_value, value_ranges$max_value),
    needs_standardization = if(value_ranges$max_value - value_ranges$min_value > 10) "Yes" else "No"
  )
}

# Create summaries for each feature set
feature_summaries <- bind_rows(
  create_feature_summary(direct_with_labels, "Direct Features"),
  create_feature_summary(link_with_labels, "Link-based Features"),
  create_feature_summary(content_with_labels, "Content-based Features")
)

# Display the summary table
feature_summaries %>%
  kable(caption = "Summary of Feature Sets") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

cat("\nFeature Set Analysis:\n")
cat("1. Direct Features (obvious features):\n",
    "- Contains count-based features with varying scales\n",
    "- Standardization needed due to different scales\n\n")
cat("2. Link-based Features (transformed):\n",
    "- Already includes some transformations (e.g., logarithmic)\n",
    "- Still shows wide value ranges\n",
    "- Standardization recommended for consistent scaling\n\n")
cat("3. Content-based Features:\n",
    "- Mix of different metrics and scales\n",
    "- Standardization needed for comparable feature importance\n")
```

> This chunk implements the data cleaning and standardization process for each feature set, handling missing values and normalizing features to ensure comparable scales.
>
> > All features were standardized using Z-score normalization (mean = 0, standard deviation = 1) via R's scale() function. As a result, the normalized values may be negative or exceed 1, which is expected and appropriate for models such as logistic regression and SVM.

```{r data-cleaning}
# Function to clean and normalize features
clean_and_normalize <- function(data) {
  # Remove ID and target columns for processing
  features <- data %>%
    select(-hostid, -is_spam) %>%
    # Remove hostname if it exists
    select(-any_of("hostname"))
  
  # Handle missing values
  features <- features %>%
    mutate_all(~ifelse(is.na(.), mean(., na.rm = TRUE), .))
  
  # Normalize features
  features_normalized <- scale(features)
  
  # Combine normalized features with ID and target
  result <- cbind(
    data %>% select(hostid, is_spam),
    as.data.frame(features_normalized)
  )
  
  # Add hostname back if it exists in original data
  if ("hostname" %in% names(data)) {
    result <- cbind(
      result %>% select(hostid),
      data %>% select(hostname),
      result %>% select(-hostid)
    )
  }
  
  result
}

# Clean and normalize each feature set
direct_clean <- clean_and_normalize(direct_with_labels)
link_clean <- clean_and_normalize(link_with_labels)
content_clean <- clean_and_normalize(content_with_labels)
```

# 3. Classification Methods Selection and Implementation

### Model Selection Strategy

For each feature set, we need to select and implement the most appropriate classification method based on the characteristics of the features and the nature of our binary classification problem. Here's our analysis and justification for each feature set:

### 1. Set 1: Direct Features (2 features)

-   **Selected Method**: Logistic Regression
-   **Justification**:
    -   Very small number of features (only 2)
    -   Simple, interpretable model suitable for low-dimensional data
    -   Efficient for binary classification
    -   Provides probability outputs for AUC calculation
    -   No need for complex feature selection or parameter tuning
-   **Additional Considerations**:
    -   Features are already standardized in preprocessing
    -   L2 regularization will be used to prevent overfitting

> This chunk implements logistic regression for the direct features set, including model training, prediction, and performance evaluation. Both single split ROC and CV AUC are computed.

```{r direct-features-model}
library(caret)
library(pROC)

set.seed(42)
direct_idx <- createDataPartition(direct_clean$is_spam, p = 0.7, list = FALSE)
direct_train <- direct_clean[direct_idx, ] %>% select(is_spam, number_of_pages, length_of_hostname)
direct_test <- direct_clean[-direct_idx, ] %>% select(is_spam, number_of_pages, length_of_hostname)

direct_model <- glm(is_spam ~ ., data = direct_train, family = "binomial")
direct_pred_prob <- predict(direct_model, direct_test, type = "response")
direct_pred_label <- ifelse(direct_pred_prob >= 0.5, 1, 0)
direct_cm <- caret::confusionMatrix(factor(direct_pred_label), factor(direct_test$is_spam), positive = "1")
direct_precision <- direct_cm$byClass["Precision"]
direct_recall <- direct_cm$byClass["Recall"]
direct_f1 <- direct_cm$byClass["F1"]

# direct_roc is used in step 4 for comparsion
direct_roc <- roc(direct_test$is_spam, direct_pred_prob)

# CV AUC (only numeric features)
# 5 æŠ˜äº¤å‰éªŒè¯ï¼ˆCross-Validationï¼‰è®¡ç®—å¹³å‡ AUC
cv_folds <- createFolds(direct_clean$is_spam, k = 5, list = TRUE, returnTrain = FALSE)
direct_auc_cv <- mean(sapply(cv_folds, function(idx) {
  train <- direct_clean[-idx, ] %>% select(is_spam, number_of_pages, length_of_hostname)
  test <- direct_clean[idx, ] %>% select(is_spam, number_of_pages, length_of_hostname)
  model <- glm(is_spam ~ ., data = train, family = "binomial")
  pred <- predict(model, test, type = "response")
  auc(roc(test$is_spam, pred, quiet=TRUE))
}))
cat("\nDirect Features 5-fold CV Mean AUC:", round(direct_auc_cv, 4), "\n")

# æ£€æŸ¥é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
cat("Direct pred label table:\n")
print(table(direct_pred_label))
cat("Direct test true label table:\n")
print(table(direct_test$is_spam))
```

### 2. Set 2: Link-based Features (138 features) 

-   **Selected Method**: Random Forest
-   **Justification**:
    -   High-dimensional feature space (138 features)
    -   Excellent performance with high-dimensional data
    -   Built-in feature importance evaluation
    -   Robust to different scales and non-linear relationships
    -   Handles class imbalance well with proper weighting
-   **Additional Considerations**:
    -   Will use class weights to address imbalance
    -   No need for additional feature scaling

> This chunk implements random forest classification for the link-based features, including single split ROC and CV AUC.

```{r link-features-model}
library(randomForest)
library(pROC)
library(caret)

# Link features: single split for ROC, CV for AUC
set.seed(42)
link_train_index <- createDataPartition(link_clean$is_spam, p = 0.7, list = FALSE)
link_train <- link_clean[link_train_index, ]
link_test <- link_clean[-link_train_index, ]

# Single split model for ROC
# å¤„ç†ä¸å¹³è¡¡
class_weights <- ifelse(link_train$is_spam == 1,
                       (1/table(link_train$is_spam)[2]) * 0.5,
                       (1/table(link_train$is_spam)[1]) * 0.5)
#å•æ¬¡è®­ç»ƒæ¨¡å‹ç”¨äºROCç”»å›¾ï¼ˆ500æ£µæ ‘ï¼‰
link_model <- randomForest(factor(is_spam) ~ ., data = link_train, weights = class_weights, ntree = 500, importance = TRUE)
link_pred_prob <- predict(link_model, link_test, type = "prob")[,2]
link_roc <- roc(link_test$is_spam, link_pred_prob)

# 5-fold cross validation for AUCï¼ˆ200æ£µæ ‘ï¼‰
cv_folds <- createFolds(link_clean$is_spam, k = 5, list = TRUE, returnTrain = FALSE)
link_auc_cv <- mean(sapply(cv_folds, function(idx) {
  train <- link_clean[-idx, ]
  test <- link_clean[idx, ]
  cw <- ifelse(train$is_spam == 1, (1/table(train$is_spam)[2]) * 0.5, (1/table(train$is_spam)[1]) * 0.5)
  model <- randomForest(factor(is_spam) ~ ., data = train, weights = cw, ntree = 200)
  pred <- predict(model, test, type = "prob")[,2]
  auc(roc(test$is_spam, pred, quiet=TRUE))
}))
cat("\nLink Features 5-fold CV Mean AUC:", round(link_auc_cv, 4), "\n")

# Link Features æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
link_pred_label <- ifelse(link_pred_prob >= 0.5, 1, 0)
link_cm <- caret::confusionMatrix(factor(link_pred_label), factor(link_test$is_spam), positive = "1")
link_precision <- link_cm$byClass["Precision"]
link_recall <- link_cm$byClass["Recall"]
link_f1 <- link_cm$byClass["F1"]
```

### 3. Set 3: Content-based Features (96 features) 

-   **Selected Method**: Support Vector Machine (SVM) with RBF kernel
-   **Justification**:
    -   Medium-dimensional feature space (96 features)
    -   Features are continuous statistical measures
    -   SVM with RBF kernel can capture non-linear relationships
    -   Effective for imbalanced datasets with proper class weights
    -   Works well with standardized features
-   **Additional Considerations**:
    -   Features are already standardized from preprocessing
    -   Will use cross-validation for hyperparameter tuning

> This chunk implements SVM classification for the content-based features, including single split ROC and CV AUC.

```{r content-features-model}
library(e1071)
library(pROC)
library(caret)

# Content features: single split for ROC, CV for AUC
set.seed(42)
content_train_index <- createDataPartition(content_clean$is_spam, p = 0.7, list = FALSE)
content_train <- content_clean[content_train_index, ]
content_test <- content_clean[-content_train_index, ]

# Exclude only hostid and hostname, keep all other features (including is_spam)
content_train_num <- content_train %>% select(-hostid, -hostname)
content_test_num <- content_test %>% select(-hostid, -hostname)


# SVMå»ºæ¨¡ï¼Œä½¿ç”¨ radial basis functionï¼ˆå¾„å‘åŸºæ ¸ï¼‰å’Œç±»ä¸å¹³è¡¡å¤„ç†
model <- svm(is_spam ~ ., data = content_train_num, kernel = "radial", cost = 1, gamma = 0.1,
             class.weights = c("0" = 1, "1" = sum(content_train_num$is_spam == 0)/sum(content_train_num$is_spam == 1)),
             probability = FALSE)

decision <- attributes(predict(model, content_test_num, decision.values = TRUE))$decision.values

# ä¿®æ­£ROCæ–¹å‘ï¼šå¦‚æœAUCå°äº0.5åˆ™è‡ªåŠ¨ç¿»è½¬decisionå€¼
content_roc <- roc(content_test_num$is_spam, as.numeric(decision), levels = c(0, 1), direction = "<", positive = 1)
if (auc(content_roc) < 0.5) {
  content_roc <- roc(content_test_num$is_spam, -as.numeric(decision), levels = c(0, 1), direction = "<", positive = 1)
}

# CV AUC (exclude only hostid, hostname)
# åˆ†fold
cv_folds <- createFolds(content_clean$is_spam, k = 5, list = TRUE, returnTrain = FALSE)

content_auc_cv <- mean(sapply(cv_folds, function(idx) {
  #å»æ‰hostid, hostname field
  train <- content_clean[-idx, ] %>% select(-hostid, -hostname)
  test <- content_clean[idx, ] %>% select(-hostid, -hostname)
  # svmæ¨¡å‹è®­ç»ƒï¼Œç²¾ç»†è°ƒæ•´éœ€è¦è°ƒæ•´cost=1, gamma=0.1ä¸¤ä¸ªå‚æ•°
  m <- svm(is_spam ~ ., data = train, kernel = "radial", cost = 1, gamma = 0.1,
           class.weights = c("0" = 1, "1" = sum(train$is_spam == 0)/sum(train$is_spam == 1)), probability = FALSE)
  dec <- attributes(predict(m, test, decision.values = TRUE))$decision.values
  auc(roc(test$is_spam, as.numeric(dec), quiet=TRUE))
}))

cat("\nContent Features 5-fold CV Mean AUC:", round(content_auc_cv, 4), "\n")

# Content Features æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
content_pred_label <- ifelse(as.numeric(decision) >= 0, 1, 0)
content_cm <- caret::confusionMatrix(factor(content_pred_label), factor(content_test_num$is_spam), positive = "1")
content_precision <- content_cm$byClass["Precision"]
content_recall <- content_cm$byClass["Recall"]
content_f1 <- content_cm$byClass["F1"]
```

# 4. Model Performance Comparison

> **Note:** AUC values in the table are from 5-fold cross-validation (CV) for each feature set, providing a robust estimate of model performance under data imbalance. ROC curves are from a single train/test split and are for visual comparison only. This approach is common in reports to balance scientific rigor and intuitive visualization.

```{r model-comparison}
# Combined ROC plot: Direct, Link, and Content (single split)
plot(direct_roc, col = "blue", main = "ROC Curves Comparison (Single Split)", lwd = 2)
lines(link_roc, col = "red", lwd = 2)
lines(content_roc$specificities, content_roc$sensitivities, col = "green", lwd = 2)
legend("bottomright", legend = c(
  paste("Direct Features (ROC, single split)"),
  paste("Link Features (ROC, single split)"),
  paste("Content Features (ROC, single split)")
), col = c("blue", "red", "green"), lwd = 2)

# Create summary table with CV AUCs
model_comparison <- data.frame(
  Feature_Set = c("Direct Features", "Link-based Features", "Content-based Features"),
  Model = c("Logistic Regression", "Random Forest", "SVM (RBF)"),
  CV_AUC = c(direct_auc_cv, link_auc_cv, content_auc_cv),
  Features = c(2, 138, 96)
)

# Display comparison table
model_comparison %>%
  arrange(desc(CV_AUC)) %>%
  mutate(CV_AUC = ifelse(is.na(CV_AUC), "NA", round(CV_AUC, 4))) %>%
  kable(caption = "Model Performance Comparison (5-fold CV AUC)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# æ£€æŸ¥é¢„æµ‹æ ‡ç­¾åˆ†å¸ƒ
cat("Direct pred label table:\n")
print(table(direct_pred_label))
cat("Direct test true label table:\n")
print(table(direct_test$is_spam))

# æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡å¯¹æ¯”è¡¨ï¼ŒNAç”¨0æ›¿ä»£
cm_comparison <- data.frame(
  Feature_Set = c("Direct Features", "Link-based Features", "Content-based Features"),
  CV_AUC = c(direct_auc_cv, link_auc_cv, content_auc_cv),
  Precision = sapply(list(direct_precision, link_precision, content_precision), function(x) ifelse(is.na(x), 0, round(x, 4))),
  Recall = sapply(list(direct_recall, link_recall, content_recall), function(x) ifelse(is.na(x), 0, round(x, 4))),
  F1 = sapply(list(direct_f1, link_f1, content_f1), function(x) ifelse(is.na(x), 0, round(x, 4)))
)

cm_comparison %>%
  kable(caption = "Model Performance Comparison (CV AUC and Classification Metrics)") %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  add_header_above(c(" " = 1, "Performance Metrics" = 4))

cat("\næ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡åˆ†æï¼š\n")
cat("- CV_AUC: 5-fold cross-validated AUCï¼Œæä¾›æ›´ç¨³å¥çš„æ€§èƒ½ä¼°è®¡\n")
cat("- Precision è¡¡é‡é¢„æµ‹ä¸º spam çš„å‡†ç¡®æ€§ï¼ŒRecall è¡¡é‡å¯¹ spam çš„å¬å›èƒ½åŠ›ï¼ŒF1 ç»¼åˆä¸¤è€…ã€‚\n")
cat("- Link-based Features é€šå¸¸åœ¨ Precisionã€Recallã€F1 ä¸Šè¡¨ç°æ›´ä¼˜ï¼ŒDirect Features å—é™äºç‰¹å¾ä¿¡æ¯ï¼ŒContent Features ä¾èµ–å†…å®¹ç‰¹å¾è´¨é‡ã€‚\n")
```

### ROC/AUC Comparison and Interpretation

The table above summarizes the predictive performance of each feature set and classification method using 5-fold cross-validated AUC as the main metric. The ROC curves provide a visual comparison of model discrimination on a single train/test split.

#### Ranking of Feature Sets by Predictive Power (AUC)

1.  **`r model_comparison$Feature_Set[which.max(model_comparison$CV_AUC)]`** (AUC = `r round(max(model_comparison$CV_AUC), 4)`) â€” *Best predictive performance*
2.  **`r model_comparison$Feature_Set[order(model_comparison$CV_AUC, decreasing=TRUE)[2]]`** (AUC = `r round(sort(model_comparison$CV_AUC, decreasing=TRUE)[2], 4)`)
3.  **`r model_comparison$Feature_Set[which.min(model_comparison$CV_AUC)]`** (AUC = `r round(min(model_comparison$CV_AUC), 4)`) â€” *Poorest predictive performance*

#### Analysis and Comparison of Results

Based on the comprehensive metrics, we can analyze the performance of each feature set and classification method:

1. **Direct Features (Logistic Regression)**
   - *Performance Metrics*:
     - CV_AUC: 0.5245 (barely better than random)
     - Precision, Recall, F1: All 0.0000
   - *Strengths*:
     - Simple and interpretable model
     - Fast training and prediction
     - Works well with small feature sets
   - *Weaknesses*:
     - Severely underperforms in this spam detection task
     - Cannot capture complex patterns with only 2 features
     - Linear model limitations in handling non-linear relationships
   - *Context*: The poor performance suggests that basic host-level features (page count and domain length) are insufficient for spam detection.

2. **Link-based Features (Random Forest)**
   - *Performance Metrics*:
     - CV_AUC: 0.7345 (moderate performance)
     - Precision: 0.3542
     - Recall: 0.1589
     - F1: 0.2194
   - *Strengths*:
     - Handles high-dimensional data (138 features) effectively
     - Captures non-linear relationships and feature interactions
     - Robust to outliers and noise
     - Built-in feature importance evaluation
   - *Weaknesses*:
     - Low recall indicates missing many spam cases
     - Moderate precision suggests false positives
     - May be computationally intensive
   - *Context*: The link structure provides valuable information for spam detection, but the model struggles with class imbalance.

3. **Content-based Features (SVM with RBF)**
   - *Performance Metrics*:
     - CV_AUC: 0.7878 (best overall performance)
     - Precision: 0.2927
     - Recall: 0.3871
     - F1: 0.3333
   - *Strengths*:
     - Best overall performance in terms of CV_AUC
     - Highest recall among all models
     - Effective at capturing non-linear patterns in text
     - Good at handling medium-dimensional data (96 features)
   - *Weaknesses*:
     - Lower precision than link-based features
     - Sensitive to parameter tuning
     - Computationally intensive for large datasets
   - *Context*: Content features provide the most discriminative power for spam detection, with SVM effectively capturing textual patterns.

#### Discussion of Classification Methods in Context

1. **Logistic Regression**
   - *Suitability for Problem*: Not suitable for this spam detection task
   - *Reasoning*: The binary nature of spam detection requires capturing complex patterns that simple linear models cannot represent
   - *Recommendation*: Should be used only as a baseline or in combination with other features

2. **Random Forest**
   - *Suitability for Problem*: Well-suited for link-based features
   - *Reasoning*: 
     - Handles the high dimensionality of link features
     - Can capture complex relationships in the link structure
     - Provides feature importance insights
   - *Recommendation*: Good choice for link-based analysis, but needs class imbalance handling

3. **SVM with RBF Kernel**
   - *Suitability for Problem*: Best performing for content-based features
   - *Reasoning*:
     - Effective at capturing non-linear patterns in text
     - Works well with standardized features
     - Good at handling the medium-dimensional content feature space
   - *Recommendation*: Optimal choice for content-based spam detection

#### Overall Findings and Recommendations

1. **Feature Set Importance**:
   - Content features are most discriminative (highest CV_AUC)
   - Link features provide complementary information
   - Direct features add little value alone

2. **Model Selection Strategy**:
   - Use SVM for content-based analysis
   - Use Random Forest for link-based analysis
   - Consider ensemble methods combining both approaches

3. **Class Imbalance Handling**:
   - All models show signs of struggling with class imbalance
   - Higher recall in content-based model suggests better handling of minority class
   - Consider additional techniques like SMOTE or class weights

4. **Practical Deployment Considerations**:
   - Content-based SVM provides best overall performance
   - Link-based Random Forest offers good precision
   - Consider computational costs in production environment

# 5. Feature Set Combinations: Model Deployment and Comparison

### 5.1 Combination 1: Obvious Features (Set 1) + Link-based Features (Set 2) 

> We combined the obvious features and link-based features to leverage both simple host-level and complex graph-based information. Random Forest is chosen for its robustness to high-dimensional data and ability to model non-linear relationships.

```{r combo1-obvious-link}
# åˆå¹¶ç‰¹å¾é›†1å’Œ2
combo1 <- direct_with_labels %>%
  select(-is_spam) %>%
  inner_join(link_with_labels, by = "hostid") %>%
  select(-hostid) # ä¿ç•™æ‰€æœ‰ç‰¹å¾å’Œis_spam

# æ ‡å‡†åŒ–ï¼ˆå»é™¤is_spamå’Œæ‰€æœ‰éæ•°å€¼å‹åˆ—ï¼‰
combo1_features <- combo1 %>% select(-is_spam) %>% select(where(is.numeric))
combo1_features_scaled <- scale(combo1_features)
combo1_clean <- cbind(is_spam = combo1$is_spam, as.data.frame(combo1_features_scaled))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
set.seed(42)
combo1_idx <- createDataPartition(combo1_clean$is_spam, p = 0.7, list = FALSE)
combo1_train <- combo1_clean[combo1_idx, ]
combo1_test <- combo1_clean[-combo1_idx, ]

# ç±»åˆ«æƒé‡
class_weights <- ifelse(combo1_train$is_spam == 1,
                       (1/table(combo1_train$is_spam)[2]) * 0.5,
                       (1/table(combo1_train$is_spam)[1]) * 0.5)

# è®­ç»ƒRandom Forest
combo1_model <- randomForest(factor(is_spam) ~ ., data = combo1_train, weights = class_weights, ntree = 500, importance = TRUE)
combo1_pred_prob <- predict(combo1_model, combo1_test, type = "prob")[,2]
combo1_roc <- roc(combo1_test$is_spam, combo1_pred_prob)

# æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
combo1_pred_label <- ifelse(combo1_pred_prob >= 0.5, 1, 0)
combo1_cm <- caret::confusionMatrix(factor(combo1_pred_label), factor(combo1_test$is_spam), positive = "1")
combo1_precision <- combo1_cm$byClass["Precision"]
combo1_recall <- combo1_cm$byClass["Recall"]
combo1_f1 <- combo1_cm$byClass["F1"]

cat("\nCombo1 (Obvious + Link) AUC:", round(auc(combo1_roc), 4), "\n")
cat("Precision:", round(combo1_precision, 4), " Recall:", round(combo1_recall, 4), " F1:", round(combo1_f1, 4), "\n")
```

### 5.2 Combination 2: Obvious Features (Set 1) + Content-based Features (Set 3) 

> We combined the obvious features and content-based features to capture both simple host-level and rich textual information. SVM with RBF kernel is chosen for its ability to model non-linear relationships in medium-dimensional data.

```{r combo2-obvious-content}
# åˆå¹¶ç‰¹å¾é›†1å’Œ3
combo2 <- direct_with_labels %>%
  select(-is_spam) %>%
  inner_join(content_with_labels, by = "hostid") %>%
  select(-hostid) # ä¿ç•™æ‰€æœ‰ç‰¹å¾å’Œis_spam

# æ ‡å‡†åŒ–ï¼ˆå»é™¤is_spamå’Œæ‰€æœ‰éæ•°å€¼å‹åˆ—ï¼‰
combo2_features <- combo2 %>% select(-is_spam) %>% select(where(is.numeric))
combo2_features_scaled <- scale(combo2_features)
combo2_clean <- cbind(is_spam = combo2$is_spam, as.data.frame(combo2_features_scaled))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
set.seed(42)
combo2_idx <- createDataPartition(combo2_clean$is_spam, p = 0.7, list = FALSE)
combo2_train <- combo2_clean[combo2_idx, ]
combo2_test <- combo2_clean[-combo2_idx, ]

# SVMå»ºæ¨¡ï¼Œå¾„å‘åŸºæ ¸+ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
combo2_train_num <- combo2_train
combo2_test_num <- combo2_test
combo2_model <- svm(is_spam ~ ., data = combo2_train_num, kernel = "radial", cost = 1, gamma = 0.1,
                    class.weights = c("0" = 1, "1" = sum(combo2_train_num$is_spam == 0)/sum(combo2_train_num$is_spam == 1)),
                    probability = FALSE)
combo2_decision <- attributes(predict(combo2_model, combo2_test_num, decision.values = TRUE))$decision.values
# ä¿®æ­£ROCæ–¹å‘
combo2_roc <- roc(combo2_test_num$is_spam, as.numeric(combo2_decision), levels = c(0, 1), direction = "<", positive = 1)
if (auc(combo2_roc) < 0.5) {
  combo2_roc <- roc(combo2_test_num$is_spam, -as.numeric(combo2_decision), levels = c(0, 1), direction = "<", positive = 1)
}
# æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
combo2_pred_label <- ifelse(as.numeric(combo2_decision) >= 0, 1, 0)
combo2_cm <- caret::confusionMatrix(factor(combo2_pred_label), factor(combo2_test_num$is_spam), positive = "1")
combo2_precision <- combo2_cm$byClass["Precision"]
combo2_recall <- combo2_cm$byClass["Recall"]
combo2_f1 <- combo2_cm$byClass["F1"]

cat("\nCombo2 (Obvious + Content) AUC:", round(auc(combo2_roc), 4), "\n")
cat("Precision:", round(combo2_precision, 4), " Recall:", round(combo2_recall, 4), " F1:", round(combo2_f1, 4), "\n")
```

### 5.3 Combination 3: Link-based Features (Set 2) + Content-based Features (Set 3) 

> We combined the link-based and content-based features to leverage both structural and textual information. Random Forest is chosen for its scalability and ability to handle mixed-type, high-dimensional data.

```{r combo3-link-content}
# åˆå¹¶ç‰¹å¾é›†2å’Œ3
combo3 <- link_with_labels %>%
  select(-is_spam) %>%
  inner_join(content_with_labels, by = "hostid") %>%
  select(-hostid) # ä¿ç•™æ‰€æœ‰ç‰¹å¾å’Œis_spam

# æ ‡å‡†åŒ–ï¼ˆå»é™¤is_spamå’Œæ‰€æœ‰éæ•°å€¼å‹åˆ—ï¼‰
combo3_features <- combo3 %>% select(-is_spam) %>% select(where(is.numeric))
combo3_features_scaled <- scale(combo3_features)
combo3_clean <- cbind(is_spam = combo3$is_spam, as.data.frame(combo3_features_scaled))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
set.seed(42)
combo3_idx <- createDataPartition(combo3_clean$is_spam, p = 0.7, list = FALSE)
combo3_train <- combo3_clean[combo3_idx, ]
combo3_test <- combo3_clean[-combo3_idx, ]

# ç±»åˆ«æƒé‡
class_weights <- ifelse(combo3_train$is_spam == 1,
                       (1/table(combo3_train$is_spam)[2]) * 0.5,
                       (1/table(combo3_train$is_spam)[1]) * 0.5)

# è®­ç»ƒRandom Forest
combo3_model <- randomForest(factor(is_spam) ~ ., data = combo3_train, weights = class_weights, ntree = 500, importance = TRUE)
combo3_pred_prob <- predict(combo3_model, combo3_test, type = "prob")[,2]
combo3_roc <- roc(combo3_test$is_spam, combo3_pred_prob)

# æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
combo3_pred_label <- ifelse(combo3_pred_prob >= 0.5, 1, 0)
combo3_cm <- caret::confusionMatrix(factor(combo3_pred_label), factor(combo3_test$is_spam), positive = "1")
combo3_precision <- combo3_cm$byClass["Precision"]
combo3_recall <- combo3_cm$byClass["Recall"]
combo3_f1 <- combo3_cm$byClass["F1"]

cat("\nCombo3 (Link + Content) AUC:", round(auc(combo3_roc), 4), "\n")
cat("Precision:", round(combo3_precision, 4), " Recall:", round(combo3_recall, 4), " F1:", round(combo3_f1, 4), "\n")
```

### 5.4 Combination 4: All Features (Set 1 + Set 2 + Set 3) 

> We combined all three feature setsâ€”obvious, link-based, and content-based featuresâ€”to leverage the full spectrum of available information. Random Forest is used for its robustness and ability to handle high-dimensional, mixed-type data.

```{r combo4-all-features}
# åˆå¹¶æ‰€æœ‰ç‰¹å¾é›†
combo4 <- direct_with_labels %>%
  select(-is_spam) %>%
  inner_join(link_with_labels, by = "hostid") %>%
  select(-is_spam) %>%
  inner_join(content_with_labels, by = "hostid") %>%
  select(-hostid) # ä¿ç•™æ‰€æœ‰ç‰¹å¾å’Œis_spam

# æ ‡å‡†åŒ–ï¼ˆå»é™¤is_spamå’Œæ‰€æœ‰éæ•°å€¼å‹åˆ—ï¼‰
combo4_features <- combo4 %>% select(-is_spam) %>% select(where(is.numeric))
combo4_features_scaled <- scale(combo4_features)
combo4_clean <- cbind(is_spam = combo4$is_spam, as.data.frame(combo4_features_scaled))

# åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
set.seed(42)
combo4_idx <- createDataPartition(combo4_clean$is_spam, p = 0.7, list = FALSE)
combo4_train <- combo4_clean[combo4_idx, ]
combo4_test <- combo4_clean[-combo4_idx, ]

# ç±»åˆ«æƒé‡
class_weights <- ifelse(combo4_train$is_spam == 1,
                       (1/table(combo4_train$is_spam)[2]) * 0.5,
                       (1/table(combo4_train$is_spam)[1]) * 0.5)

# è®­ç»ƒRandom Forest
combo4_model <- randomForest(factor(is_spam) ~ ., data = combo4_train, weights = class_weights, ntree = 500, importance = TRUE)
combo4_pred_prob <- predict(combo4_model, combo4_test, type = "prob")[,2]
combo4_roc <- roc(combo4_test$is_spam, combo4_pred_prob)

# æ··æ·†çŸ©é˜µå’ŒæŒ‡æ ‡
combo4_pred_label <- ifelse(combo4_pred_prob >= 0.5, 1, 0)
combo4_cm <- caret::confusionMatrix(factor(combo4_pred_label), factor(combo4_test$is_spam), positive = "1")
combo4_precision <- combo4_cm$byClass["Precision"]
combo4_recall <- combo4_cm$byClass["Recall"]
combo4_f1 <- combo4_cm$byClass["F1"]

cat("\nCombo4 (All Features) AUC:", round(auc(combo4_roc), 4), "\n")
cat("Precision:", round(combo4_precision, 4), " Recall:", round(combo4_recall, 4), " F1:", round(combo4_f1, 4), "\n")
```

### 5.5 Qualitative Comparison of Feature Set Combinations

> The table below summarizes the predictive performance of each feature set combination using AUC, Precision, Recall, and F1-score. This is followed by a qualitative analysis of the results, highlighting the strengths and weaknesses of each approach and the impact of feature set integration.

```{r combo-comparison-table}
# Create ROC plot for all combinations
plot(combo1_roc, col = "blue", main = "ROC Curves Comparison of Feature Set Combinations", lwd = 2)
lines(combo2_roc$specificities, combo2_roc$sensitivities, col = "red", lwd = 2)
lines(combo3_roc$specificities, combo3_roc$sensitivities, col = "green", lwd = 2)
lines(combo4_roc$specificities, combo4_roc$sensitivities, col = "purple", lwd = 2)
legend("bottomright", legend = c(
  paste("Obvious + Link (RF)"),
  paste("Obvious + Content (SVM)"),
  paste("Link + Content (RF)"),
  paste("All Features (RF)")
), col = c("blue", "red", "green", "purple"), lwd = 2)

# Create comparison table
combo_comparison <- data.frame(
  Combination = c(
    "Obvious + Link",
    "Obvious + Content",
    "Link + Content",
    "All Features"
  ),
  Model = c(
    "Random Forest",
    "SVM (RBF)",
    "Random Forest",
    "Random Forest"
  ),
  AUC = c(
    round(auc(combo1_roc), 4),
    round(auc(combo2_roc), 4),
    round(auc(combo3_roc), 4),
    round(auc(combo4_roc), 4)
  ),
  Precision = c(
    round(combo1_precision, 4),
    round(combo2_precision, 4),
    round(combo3_precision, 4),
    round(combo4_precision, 4)
  ),
  Recall = c(
    round(combo1_recall, 4),
    round(combo2_recall, 4),
    round(combo3_recall, 4),
    round(combo4_recall, 4)
  ),
  F1 = c(
    round(combo1_f1, 4),
    round(combo2_f1, 4),
    round(combo3_f1, 4),
    round(combo4_f1, 4)
  )
)

combo_comparison %>%
  kable(caption = "Performance Comparison of Feature Set Combinations (Single Split)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

#### Qualitative Analysis

-   **Obvious + Link** This combination utilizes both simple site-level metrics and structural link data. The AUC of 0.7662 is modest, but the recall (0.1402) is very low, meaning the model misses many spam cases. While Random Forest handles link features well, the obvious features contribute little, and the overall spam detection is weak.

-   **Obvious + Content** The SVM (RBF) model captures subtle, non-linear textual patterns in content. Although the precision (0.3000) is slightly lower, the recall improves significantly to 0.3871, resulting in the highest F1-score (0.3380) among all combinations. This suggests it's relatively better at identifying spam correctly, even with fewer features.

-   **Link + Content** This combination balances structural and content signals. Random Forest delivers the second-highest AUC (0.8240) and a better balance of precision (0.5152) and recall (0.1828) compared to previous sets. It shows robustness across spam strategies, although F1-score (0.2698) remains slightly behind SVM.

-   **All Features** Using the full feature set yields the best overall AUC (0.8249) and the highest precision (0.5714). Recall (0.2151) improves over other Random Forest models, giving a strong F1-score of 0.3125. Although not the top in F1, this approach ensures stable, high-confidence predictions and is suitable when computational cost is acceptable.

â¸»

ğŸ” **General Insights:** - Combining Link + Content is crucial â€” this pairing consistently outperforms combinations that include only one of the two. - SVM excels in recall and F1 when using content features, showing its strength in uncovering non-linear patterns. - Random Forest remains a solid choice for high-dimensional mixed-type features, offering stable AUC and precision. - If spam recall is a priority, SVM with content may be preferred; if balanced performance is key, Random Forest with all features is recommended.
